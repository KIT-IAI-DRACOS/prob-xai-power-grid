{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import time \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import properscoring as ps\n",
    "from datetime import datetime\n",
    "\n",
    "import joblib\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from typing import List, NamedTuple\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"./\")\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../..\")\n",
    "from utils.tabnet_proba import TabNetRegressorProba\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = [\n",
    "    \"gen_other\",\n",
    "    \"gen_solar\",\n",
    "    \"gen_wind_on\",\n",
    "    \"gen_waste\",\n",
    "    \"gen_nuclear\",\n",
    "    \"gen_biomass\",\n",
    "    \"gen_gas\",\n",
    "    \"gen_run_off_hydro\",\n",
    "    \"gen_oil\",\n",
    "    \"gen_pumped_hydro\",\n",
    "    \"gen_other_renew\",\n",
    "    \"gen_reservoir_hydro\",\n",
    "    \"gen_hard_coal\",\n",
    "    \"gen_wind_off\",\n",
    "    \"gen_geothermal\",\n",
    "    \"gen_lignite\",\n",
    "    \"load\",\n",
    "    \"gen_coal_gas\",\n",
    "    \"total_gen\",\n",
    "    \"synchronous_gen\",\n",
    "    \"load_ramp\",\n",
    "    \"total_gen_ramp\",\n",
    "    \"other_ramp\",\n",
    "    \"solar_ramp\",\n",
    "    \"wind_on_ramp\",\n",
    "    \"waste_ramp\",\n",
    "    \"nuclear_ramp\",\n",
    "    \"biomass_ramp\",\n",
    "    \"gas_ramp\",\n",
    "    \"run_off_hydro_ramp\",\n",
    "    \"oil_ramp\",\n",
    "    \"pumped_hydro_ramp\",\n",
    "    \"other_renew_ramp\",\n",
    "    \"reservoir_hydro_ramp\",\n",
    "    \"hard_coal_ramp\",\n",
    "    \"wind_off_ramp\",\n",
    "    \"geothermal_ramp\",\n",
    "    \"lignite_ramp\",\n",
    "    \"coal_gas_ramp\",\n",
    "    \"forecast_error_wind_on\",\n",
    "    \"forecast_error_wind_off\",\n",
    "    \"forecast_error_solar\",\n",
    "    \"forecast_error_total_gen\",\n",
    "    \"forecast_error_load\",\n",
    "    \"forecast_error_load_ramp\",\n",
    "    \"forecast_error_total_gen_ramp\",\n",
    "    \"forecast_error_wind_off_ramp\",\n",
    "    \"forecast_error_wind_on_ramp\",\n",
    "    \"forecast_error_solar_ramp\",\n",
    "    \"solar_day_ahead\",\n",
    "    \"wind_on_day_ahead\",\n",
    "    \"scheduled_gen_total\",\n",
    "    \"prices_day_ahead\",\n",
    "    \"load_day_ahead\",\n",
    "    \"wind_off_day_ahead\",\n",
    "    \"month\",\n",
    "    \"weekday\",\n",
    "    \"hour\",\n",
    "    \"load_ramp_day_ahead\",\n",
    "    \"total_gen_ramp_day_ahead\",\n",
    "    \"wind_off_ramp_day_ahead\",\n",
    "    \"wind_on_ramp_day_ahead\",\n",
    "    \"solar_ramp_day_ahead\",\n",
    "    \"price_ramp_day_ahead\",\n",
    "    \"gen_fossil_peat\",\n",
    "    \"fossil_peat_ramp\",\n",
    "    \"residual\",\n",
    "]\n",
    "\n",
    "\n",
    "input_col_names = [\n",
    "    \"Generation other\",\n",
    "    \"Solar generation\",\n",
    "    \"Onshore wind generation\",\n",
    "    \"Waste generation\",\n",
    "    \"Nuclear generation\",\n",
    "    \"Biomass generation\",\n",
    "    \"Gas generation\",\n",
    "    \"Run-off-river hydro generation\",\n",
    "    \"Oil generation\",\n",
    "    \"Pumped hydro generation\",\n",
    "    \"Other renewable generation\",\n",
    "    \"Reservoir hydro generation\",\n",
    "    \"Hard coal generation\",\n",
    "    \"Wind offshore generation\",\n",
    "    \"Geothermal generation\",\n",
    "    \"Lignite generation\",\n",
    "    \"Load\",\n",
    "    \"Coal gas generation\",\n",
    "    \"Total generation\",\n",
    "    \"Synchronous generation\",\n",
    "    \"Load ramp\",\n",
    "    \"Total generation ramp\",\n",
    "    \"Other ramp\",\n",
    "    \"Solar ramp\",\n",
    "    \"Onshore wind ramp\",\n",
    "    \"Waste ramp\",\n",
    "    \"Nuclear ramp\",\n",
    "    \"Biomass ramp\",\n",
    "    \"Gas ramp\",\n",
    "    \"Run-off-river hydro ramp\",\n",
    "    \"Oil ramp\",\n",
    "    \"Pumped hydro ramp\",\n",
    "    \"Other renewable ramp\",\n",
    "    \"Reservoir hydro ramp\",\n",
    "    \"Hard coal ramp\",\n",
    "    \"Offshore wind ramp\",\n",
    "    \"geothermal_ramp\",\n",
    "    \"Lignite ramp\",\n",
    "    \"Coal gas ramp\",\n",
    "    \"Forecast error onshore wind\",\n",
    "    \"Forecast error offshore wind\",\n",
    "    \"Forecast error solar\",\n",
    "    \"Forecast error total generation\",\n",
    "    \"Forecast error load\",\n",
    "    \"Forecast error load ramp\",\n",
    "    \"Forecast error generation ramp\",\n",
    "    \"Forecast error offshore wind ramp\",\n",
    "    \"Forecast error onshore wind ramp\",\n",
    "    \"Forecast error solar ramp\",\n",
    "    \"Solar day-ahead\",\n",
    "    \"Onshore wind day-ahead\",\n",
    "    \"Scheduled generation\",\n",
    "    \"Prices day-ahead\",\n",
    "    \"Load day-ahead\",\n",
    "    \"Offshore wind day-ahead\",\n",
    "    \"Month\",\n",
    "    \"Weekday\",\n",
    "    \"Hour\",\n",
    "    \"Load ramp day-ahead\",\n",
    "    \"Generation ramp day-ahead\",\n",
    "    \"Offshore wind ramp day-ahead\",\n",
    "    \"Onshore wind ramp day-ahead\",\n",
    "    \"Solar ramp day-ahead\",\n",
    "    \"Price ramp day-ahead\",\n",
    "    \"Fossil peat generation\",\n",
    "    \"Fossil peat ramp\",\n",
    "    \"Residual\",\n",
    "]\n",
    "\n",
    "input_col_names = dict(zip(input_cols, input_col_names))\n",
    "#input_col_names_units = dict(zip(input_cols, input_col_names_units))\n",
    "#input_col_names_units_general = dict(zip(input_cols, input_col_names_units_general))\n",
    "\n",
    "input_rescale_factors = pd.Series(index=input_cols, data=1 / 1000)\n",
    "input_rescale_factors.loc[\n",
    "    [\"weekday\", \"hour\", \"month\", \"prices_day_ahead\", \"price_ramp_day_ahead\"]\n",
    "] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(NamedTuple):\n",
    "    data_version: str = \"2024-05-19\"\n",
    "    res_version: str = \"2024-06-24\"\n",
    "    model_type: str = \"_full\"\n",
    "    model_combination: str = \"ngb\"\n",
    "    model_name: str = \"NGBoost\"\n",
    "    predictors_prob: List[str] = [\"baseline\", \"predictions\"]\n",
    "    model_names_prob: List[str] =[\"Baseline\", \"NGBoost (full model)\"],\n",
    "    model_names_det: List[str] =[\"Daily profile\", \"NGBoost (full model)\"],    \n",
    "    predictors_det: List[str] = [\"daily_profile\", \"predictions\"]\n",
    "    scaler_str: str = \"yeo_johnson\"\n",
    "    scaled: str = \"_scaled\"\n",
    "    explanations: str =\"_partition\" # for tabnet proba it can be \"_partition\" because of the two explainers\n",
    "\n",
    "config_ngb = Config()\n",
    "\n",
    "config_tabnet_proba = Config(\n",
    "    res_version=\"2024-08-27\",\n",
    "    model_combination=\"tabnet_proba_final\",\n",
    "    model_name = \"TabnetProba\",\n",
    "    model_names_det=[\"Daily profile\", \"TabNetProba\"],\n",
    "    explanations=\"_partition\"\n",
    ")\n",
    "\n",
    "config_xgb = Config(\n",
    "    res_version = \"2024-05-19\",\n",
    "    model_combination= \"xgb\",\n",
    "    model_name = \"XGBoost\",\n",
    "    predictors_det = [\"daily_profile\", \"predictions\"],\n",
    "    model_names_det = [\"Daily profile\", \"XGBoost\"],\n",
    "    scaler_str = \"\",\n",
    "    scaled = \"\",\n",
    "    explanations=\"_partition\"\n",
    ")\n",
    "\n",
    "config_tabnet = Config(\n",
    "    res_version = \"2024-09-20\",\n",
    "    model_combination= \"tabnet_det_rs\",\n",
    "    model_name = \"TabNet\",\n",
    "    predictors_det = [\"daily_profile\", \"predictions\"],\n",
    "    model_names_det = [\"Daily profile\", \"TabNet\"],\n",
    "    scaler_str = \"yeo_johnson\",\n",
    "    scaled = \"_scaled\",\n",
    "    explanations=\"_partition\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../data/2020-2024/{}/version_{}/yeo_johnson/\"\n",
    "explain_folder = \"../explanations/{}/version_{}_{}/target_{}/explanations{}/\"\n",
    "fit_folder = \"../results/model_fit/{}/version_{}_{}\"+ \"/{}/\" + \"target_{}/\"\n",
    "\n",
    "scaled_str = \"_scaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_names = [\"Continental Europe\", \"Nordic\"]\n",
    "area_colors = [\"C0\", \"C1\"]\n",
    "model_colors = ['#DF9B1B', '#009682', '#4664AA', '#8CB6C3',   '#A22223', '#20b2aa']\n",
    "targets = [\"f_rocof\", \"f_ext\", \"f_msd\", \"f_integral\"]\n",
    "target_names = {\"f_rocof\": \"RoCoF\",\n",
    "                \"f_ext\": \"Nadir\",\n",
    "                \"f_msd\": \"MSD\",\n",
    "                \"f_integral\": \"Integral\"}\n",
    "target_units = {\"f_rocof\": \"Hz/s\",\n",
    "                \"f_ext\": \"Hz\",\n",
    "                \"f_msd\": \"\",\n",
    "                \"f_integral\": \"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1 - Frequency Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2. Performance Comparison of Deterministc Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Create Function and also use for figure 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set LaTeX for rendering text in Matplotlib\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "# Enable LaTeX text rendering and set a larger default font size\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif', size=16)  # Increase font size\n",
    "plt.rc('axes', titlesize=20)  # Title font size\n",
    "plt.rc('axes', labelsize=18)  # X and Y axis label font size\n",
    "plt.rc('legend', fontsize=14)  # Legend font size\n",
    "plt.rc('xtick', labelsize=16)  # X-axis tick label size\n",
    "plt.rc('ytick', labelsize=16)  # Y-axis tick label size\n",
    "\n",
    "configs = [\n",
    "    config_xgboost,\n",
    "    config_tabnet,\n",
    "    config_ngb,\n",
    "    #config_tabnet_det,\n",
    "    \n",
    "    config_tabnet_proba,\n",
    "]\n",
    "\n",
    "# Define paths\n",
    "version_folder = \"../../data/2020-2024/{}/version_{}/\" + config_ngb.scaler_str + \"/\"\n",
    "version_folder_xgb = \"../../data/2020-2024/{}/version_{}/\"\n",
    "\n",
    "fit_folder_template = (\n",
    "    \"../../results/model_fit/{}/version_{}_\" + \"{}\" + \"/\" + \"{}\" + \"/target_{}/\"\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 6), sharey=True)\n",
    "\n",
    "#model_colors = ['#DF9B1B', '#009682', '#4664AA', '#8CB6C3',   '#A22223', '#20b2aa']\n",
    "\"\"\"\n",
    "model_colors = [\"#9b19f5\", \"#0bb4ff\",#\"#e6d800\",\n",
    "                \"#ffa300\", \"#e60049\", \"#4A2377\",\n",
    "                #\"#50e991\", \"#e6d800\", \n",
    "                  #\"#dc0ab4\", \n",
    "                  \"#b3d4ff\", \"#00bfa0\"]\n",
    "\n",
    "model_colors_2 = [\"#f9e858\", \"#003a7d\", \"#008dff\", \"#ff73b6\", \"#b33dc6\"]\n",
    "\"\"\"\n",
    "for i, area in enumerate(areas):\n",
    "    scores = pd.DataFrame(\n",
    "        index=targets,\n",
    "        columns=[f\"{config.model_combination}\" for config in configs].append(\n",
    "            \"daily_profile\"\n",
    "        ),\n",
    "        dtype=\"float\",\n",
    "    )\n",
    "\n",
    "    for m, config in enumerate(configs):\n",
    "        for j, targ in enumerate(targets):\n",
    "            try:\n",
    "                if m == 0:\n",
    "                    y_test = pd.read_hdf(\n",
    "                    version_folder_xgb.format(area, config.data_version)\n",
    "                    + f\"y_test.h5\"\n",
    "                ).loc[:, targ]\n",
    "                else:\n",
    "                    y_test = pd.read_hdf(\n",
    "                        version_folder.format(area, config.data_version)\n",
    "                        + f\"y_test{config.scaled}.h5\"\n",
    "                    ).loc[:, targ]\n",
    "                fit_folder = fit_folder_template.format(\n",
    "                    area,\n",
    "                    config.res_version,\n",
    "                    config.model_combination,\n",
    "                    config.scaler_str,\n",
    "                    targ,\n",
    "                )\n",
    "                y_pred = pd.read_hdf(fit_folder + \"y_pred.h5\")\n",
    "                if m == 0: # then this is xgboost\n",
    "                    y_pred.rename(columns={\"gtb_full\": \"predictions\"}, inplace=True)\n",
    "                    scores.loc[targ, \"daily_profile\"] = r2_score(\n",
    "                        y_test, y_pred[\"daily_profile\"]\n",
    "                    )\n",
    "                else:\n",
    "                    y_pred.rename(\n",
    "                        columns={f\"{targ}_prediction\": \"predictions\"}, inplace=True\n",
    "                    )\n",
    "                    \n",
    "                scores.loc[targ, f\"{config.model_combination}\"] = r2_score(\n",
    "                    y_test, y_pred[\"predictions\"]\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\n",
    "                    f\"Error processing {config.model_combination} for {targ} in {area}: {e}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "    # Replace NaN values with a placeholder (e.g., 0) for plotting\n",
    "    scores = scores.fillna(0)\n",
    "\n",
    "    # Use LaTeX for axis labels and titles\n",
    "    scores.rename(columns={\"daily_profile\": \"Daily Profile\", \"xgb\": \"XGBoost\", \"ngb\": \"NGBoost\", \"tabnet_tuner\": \"TabNet\", \"tabnet_proba_final\":\"TabNetProba\"}, inplace=True)\n",
    "    scores.plot.bar(\n",
    "        ylim=[-0.02, scores.max().max() + 0.05],\n",
    "        width=0.85,\n",
    "        ax=ax[i],\n",
    "        legend=False,\n",
    "        #color=model_colors_2,\n",
    "    )\n",
    "    print(scores)\n",
    "    ax[i].set_xticks(np.arange(len(targets)))\n",
    "    ax[i].set_xticklabels(target_names, rotation=30, ha=\"right\")\n",
    "    ax[i].grid(axis=\"y\")\n",
    "    ax[i].set_title(area_names[i])\n",
    "\n",
    "    relative_increase = scores.div(scores.iloc[:, 0], axis=0)\n",
    "\n",
    "    \"\"\"\n",
    "    for j, targ in enumerate(targets):\n",
    "        try:\n",
    "            ml_gain = relative_increase.loc[targ].max()\n",
    "            perf_val = scores.loc[targ, relative_increase.loc[targ].idxmax()]\n",
    "            ax[i].annotate(\n",
    "                r\"$\\times$ {:.1f}\".format(ml_gain),\n",
    "                (j - i * 0.1 - 0.2, perf_val + 0.015),\n",
    "                fontsize=8,\n",
    "                c=shap.plots.colors.gray_rgb,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error annotating {targ} in {area}: {e}\")\n",
    "            continue\n",
    "    \"\"\"\n",
    "\n",
    "ax[0].set_ylabel(r\"R$^2$ Score\")  # LaTeX for R^2\n",
    "ax[0].set_ylim([-0.01, 0.82])\n",
    "ax[0].set_yticks(np.arange(0, 1.1, 0.2))  # Set y-axis ticks in 0.2 steps\n",
    "ax[1].set_yticks(np.arange(0, 1.1, 0.2))  # Set y-axis ticks in 0.2 steps\n",
    "plt.tight_layout()\n",
    "\n",
    "\"\"\"\n",
    "leg1 = ax[1].legend(bbox_to_anchor=(0.38, -0.2), ncol=3, title=r\"Models\")  # LaTeX for title\n",
    "annot_text = plt.plot(\n",
    "    -10, 0, marker=r\"$\\times$ 9.9\", ms=22, lw=0, c=shap.plots.colors.gray_rgb\n",
    ")\n",
    "leg2 = ax[1].legend(\n",
    "    annot_text, [r\"Gain over daily \\\\ profile\"], bbox_to_anchor=(0.9, -0.25)  # LaTeX for annotation\n",
    ")\n",
    "ax[1].add_artist(leg1)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "plt.legend(title=r\"Models\")\n",
    "#plt.savefig(\"../../results/figures/\" + 'r2_score_overview_proba.pdf', bbox_inches='tight')\n",
    "#plt.savefig(\"../../results/figures/\" + 'r2_score_xgb_tabnet.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3. Actual vs predicted RoCoF Examples\n",
    "## TODO: extract exactly the two days from the paper plot and not a fully random period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_df(config, area, target):\n",
    "    X_train = pd.read_hdf(data_folder.format(area, config.data_version) + \"X_train_full_scaled.h5\")\n",
    "    X_test = pd.read_hdf(data_folder.format(area, config.data_version) + \"X_test_full_scaled.h5\")\n",
    "\n",
    "    y_train = pd.read_hdf(data_folder.format(area, config.data_version)+ f\"y_train{config.scaled}.h5\").loc[:, target]\n",
    "\n",
    "    daily_profile = y_train.groupby(X_train.index.time).mean()\n",
    "    daily_profile_std = y_train.groupby(X_train.index.time).std()\n",
    "\n",
    "    y_pred = pd.read_hdf(fit_folder.format(area, config.res_version, config.model_combination, config.scaler_str, target) + \"y_pred.h5\")\n",
    "    y_train_pred = pd.read_hdf(data_folder.format(area, config.data_version)+ f\"y_train{config.scaled}.h5\").loc[:, [target]]\n",
    "\n",
    "    y_pred[\"daily_profile\"] = [daily_profile[time] for time in X_test.index.time]\n",
    "    y_pred[\"daily_profile_std\"] = [daily_profile_std[time] for time in X_test.index.time]\n",
    "    y_pred[\"crps_daily_profile\"] =  ps.crps_gaussian(y_pred[f\"{target}\"],\n",
    "                                                        np.array(y_pred[\"daily_profile\"]),\n",
    "                                                        np.array(y_pred[\"daily_profile_std\"]))\n",
    "    y_train_pred[\"daily_profile\"] = [daily_profile[time] for time in X_train.index.time]\n",
    "    y_train_pred[\"daily_profile_std\"] = [daily_profile_std[time] for time in X_train.index.time]\n",
    "    y_train_pred[\"crps_daily_profile\"] =  ps.crps_gaussian(y_train_pred[f\"{target}\"],\n",
    "                                                        np.array(y_train_pred[\"daily_profile\"]),\n",
    "                                                        np.array(y_train_pred[\"daily_profile_std\"]))\n",
    "    \n",
    "    if config.model_combination == \"ngb\":\n",
    "        ngb_model = joblib.load(fit_folder.format(area, config_ngb.res_version, config_ngb.model_combination, config_ngb.scaler_str, target) + \"best_ngb_model.pkl\")\n",
    "        y_train_pred_dist = ngb_model.pred_dist(X_train)\n",
    "        y_pred_dist = ngb_model.pred_dist(X_test)\n",
    "        y_pred[\"crps_model\"] = ps.crps_gaussian(y_pred[f\"{target}\"], y_pred_dist.params[\"loc\"], y_pred_dist.params[\"scale\"])\n",
    "\n",
    "        y_train_pred[\"predictions_upper\"] = y_train_pred_dist.dist.interval(0.95)[1]\n",
    "        y_train_pred[\"predictions_lower\"] = y_train_pred_dist.dist.interval(0.95)[0]\n",
    "        y_train_pred[\"predictions\"] = y_train_pred_dist.loc\n",
    "        \n",
    "        y_train_pred[\"crps_model\"] = ps.crps_gaussian(y_train_pred[f\"{target}\"], y_train_pred_dist.params[\"loc\"], y_train_pred_dist.params[\"scale\"])\n",
    "    \n",
    "    elif config.model_combination == \"tabnet_proba_final\":\n",
    "        confidence_level = 0.95\n",
    "        z_score = 1.96  # z-score for 95% confidence interval\n",
    "        \n",
    "        y_pred.rename(columns={f\"{target}_prediction\": \"predictions\"}, inplace=True)\n",
    "        \n",
    "        reg = TabNetRegressorProba()\n",
    "        reg.load_model(\n",
    "            filepath=fit_folder.format(area, config.res_version, config.model_combination, config.scaler_str, target) + \"best_model_params.zip\")\n",
    "        \n",
    "        predictions = reg.predict(X_train.values)\n",
    "        y_train_pred[\"predictions\"] = predictions[:, 0]\n",
    "        y_train_pred[f\"{target}_cov_pred\"] = predictions[:, 1]\n",
    "        \n",
    "        for df in [y_pred, y_train_pred]:\n",
    "            std_dev = np.sqrt(df[f\"{target}_cov_pred\"])\n",
    "            df[\"predictions_lower\"] = df[\"predictions\"] - z_score * std_dev\n",
    "            df[\"predictions_upper\"] = df[\"predictions\"] + z_score * std_dev\n",
    "            df[\"crps_model\"] = ps.crps_gaussian(df[f\"{target}\"],\n",
    "                                                np.array(df[\"predictions\"]),\n",
    "                                                np.array(std_dev)\n",
    "                                            )\n",
    "\n",
    "\n",
    "    y_train_pred_inverse = inverse_transform(y_train_pred, area, target, config)\n",
    "    y_pred_inverse = inverse_transform(y_pred, area, target, config)\n",
    "\n",
    "    y_pred_inverse['source'] = 'prediction'\n",
    "    y_train_pred_inverse['source'] = 'training'\n",
    "\n",
    "    y_inverse_full = pd.concat([y_pred_inverse, y_train_pred_inverse], axis=0)\n",
    "    z_score = 1.96\n",
    "    y_inverse_full[\"daily_profile_lower\"] = y_inverse_full[\"daily_profile\"] - z_score * y_inverse_full[\"daily_profile_std\"] \n",
    "    y_inverse_full[\"daily_profile_upper\"] = y_inverse_full[\"daily_profile\"] + z_score * y_inverse_full[\"daily_profile_std\"] \n",
    "\n",
    "    ##\n",
    "    residual_std = np.std(y_inverse_full[f\"{target}\"] - y_inverse_full[\"daily_profile\"])\n",
    "\n",
    "    # Or if you already have this value from your model as \"observation_noise\" or similar\n",
    "\n",
    "    # Calculate prediction interval\n",
    "    prediction_std = np.sqrt(y_inverse_full[\"daily_profile_std\"]**2 + residual_std**2)\n",
    "\n",
    "    # Now calculate the prediction intervals\n",
    "    y_inverse_full[\"daily_profile_lower\"] = y_inverse_full[\"daily_profile\"] - z_score * prediction_std\n",
    "    y_inverse_full[\"daily_profile_upper\"] = y_inverse_full[\"daily_profile\"] + z_score * prediction_std\n",
    "    ##\n",
    "    \n",
    "    y_inverse_full.drop([\"mean_predictor\", \"std_predictor\"], axis=1, inplace=True)\n",
    "    return y_inverse_full.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df, save=False, filename=\"plot.pdf\", dpi=300, var_name=\"f_rocof\", model_name=None, daily_profile_crps=None, model_crps=None):\n",
    "    \"\"\"\n",
    "    Plot time series results with daily profile and predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing the data to plot\n",
    "    save : bool, default=False\n",
    "        Whether to save the figure to disk\n",
    "    filename : str, default=\"plot.pdf\"\n",
    "        Filename to use when saving the figure (can be .pdf, .png, etc.)\n",
    "    dpi : int, default=300\n",
    "        Resolution for the saved figure\n",
    "    var_name : str, default=\"RoCoF\"\n",
    "        Name of the variable being plotted\n",
    "    model_name : str, default=None\n",
    "        Name of the model to display as a small title\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.dates as mdates\n",
    "    from matplotlib.patches import Patch\n",
    "    import datetime\n",
    "    \n",
    "    # Enable LaTeX rendering\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Computer Modern Roman\"],\n",
    "        \"font.size\": 14,\n",
    "        \"axes.labelsize\": 16,\n",
    "        \"legend.fontsize\": 14,\n",
    "        \"xtick.labelsize\": 14,\n",
    "        \"ytick.labelsize\": 14\n",
    "    })\n",
    "    \n",
    "    # Generate automatic ylabel if none provided\n",
    "    ylabel_var = target_names[var_name]\n",
    "    unit = target_units[var_name]\n",
    "    if unit == \"\":\n",
    "        ylabel = r\"$\\textit{\" + ylabel_var + r\"~Value}$\"\n",
    "    else:\n",
    "        ylabel = r\"$\\textit{\" + ylabel_var + r\"~Value}~[\\mathrm{\" + unit + r\"}]$\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    \n",
    "    # Make sure the dataframe is sorted by index (timestamp)\n",
    "    df = df.sort_index()\n",
    "    # Map the column names assuming they follow the pattern used in the original code\n",
    "    actual_col = f\"{var_name}\"\n",
    "    pred_col = \"predictions\"\n",
    "    pred_lower_col = \"predictions_lower\"\n",
    "    pred_upper_col = \"predictions_upper\"\n",
    "    daily_profile_col = \"daily_profile\"\n",
    "    daily_profile_lower_col = \"daily_profile_lower\"\n",
    "    daily_profile_upper_col = \"daily_profile_upper\"\n",
    "\n",
    "    daily_profile_crps = df.crps_daily_profile.mean()\n",
    "    model_crps = df.crps_model.mean()\n",
    "\n",
    "    \n",
    "    # Plot the daily profile baseline with shading\n",
    "    plt.plot(df.index, df[daily_profile_col],\n",
    "             label='Daily Profile',\n",
    "             color='purple',\n",
    "             linestyle='-',\n",
    "             alpha=0.7)\n",
    "    \n",
    "    # Add shading for daily profile\n",
    "    plt.fill_between(df.index, \n",
    "                     df[daily_profile_lower_col], \n",
    "                     df[daily_profile_upper_col],\n",
    "                     color='purple', alpha=0.1)\n",
    "    \n",
    "    # Plot actual values as a single continuous line for the entire dataset\n",
    "    plt.plot(df.index, df[actual_col], \n",
    "             color='g', linestyle='-', linewidth=1.5, \n",
    "             alpha=0.8, label='_nolegend_')\n",
    "    \n",
    "    # Add a single line with the correct style for the legend\n",
    "    var_name = var_name[2:]\n",
    "    f_label = r'$f_{' + var_name + r'}$ actual'\n",
    "    plt.plot([], [], color='g', linestyle='-', linewidth=1.5,label=f_label)\n",
    "    \n",
    "    # Plot a single line for all predictions regardless of source\n",
    "    # Use legend_only plot for the legend entry\n",
    "    f_pred_label = r'$f_{' + var_name + r'}$ predicted'\n",
    "    plt.plot([], [], color='b', lw=1.5, label=f_pred_label)\n",
    "    \n",
    "    # Plot the continuous prediction line for the entire dataset\n",
    "    plt.plot(df.index, df[pred_col], color='b', lw=1.5, label='_nolegend_')\n",
    "    \n",
    "    # Now handle the confidence intervals with different transparencies for training/prediction\n",
    "    for start, end, source in get_chunks(df):\n",
    "        chunk_data = df.loc[start:end]\n",
    "        if source == 'training':\n",
    "            # Confidence intervals for training\n",
    "            plt.fill_between(chunk_data.index,\n",
    "                            chunk_data[pred_lower_col],\n",
    "                            chunk_data[pred_upper_col],\n",
    "                            color='cornflowerblue', alpha=0.2)\n",
    "        else:\n",
    "            # Confidence intervals for test\n",
    "            plt.fill_between(chunk_data.index,\n",
    "                            chunk_data[pred_lower_col],\n",
    "                            chunk_data[pred_upper_col],\n",
    "                            color='cornflowerblue', alpha=0.5)\n",
    "    \n",
    "    # Add patches for the shadings\n",
    "    daily_profile_patch = Patch(color='purple', alpha=0.2, label='Daily Profile PI')\n",
    "    model_ci_patch_train = Patch(color='cornflowerblue', alpha=0.2, label=r'95\\% PI (training)')\n",
    "    model_ci_patch_test = Patch(color='cornflowerblue', alpha=0.5, label=r'95\\% PI (test)')\n",
    "    \n",
    "    # Create a simpler legend with fewer elements but still separate CI entries\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    \n",
    "    # Add the confidence interval patches to the legend\n",
    "    handles.append(daily_profile_patch)\n",
    "    handles.append(model_ci_patch_train)\n",
    "    handles.append(model_ci_patch_test)\n",
    "    labels.append(r'95\\% PI (daily profile)')\n",
    "    labels.append(r'95\\% PI (training)')\n",
    "    labels.append(r'95\\% PI (test)')\n",
    "    \n",
    "    # Create the new legend and position it outside the plot\n",
    "    # Place the legend outside the plot area at the bottom\n",
    "    ax.legend(handles, labels, fontsize=12, loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
    "    \n",
    "    # Extract unique dates from the index\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Get the date part only from the datetime index and find unique dates\n",
    "    date_strings = [d.strftime('%Y-%m-%d') for d in df.index]\n",
    "    unique_date_strs = list(set(date_strings))\n",
    "    unique_dates = [datetime.strptime(d, '%Y-%m-%d') for d in unique_date_strs]\n",
    "    unique_dates.sort()  # Sort chronologically\n",
    "    print(unique_dates)\n",
    "    # Set these specific dates as tick locations\n",
    "    ax.set_xticks(unique_dates)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    #ax.tick_params(axis='x', labelrotation=40)\n",
    "    \n",
    "    # LaTeX formatted labels\n",
    "    plt.ylabel(ylabel, size=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Create a box for model information and metrics in the top right\n",
    "    info_text = \"\"\n",
    "    if model_name:\n",
    "        info_text += r\"\\textit{Model: \" + model_name + \"}\"\n",
    "    \n",
    "    # Add metrics information if provided\n",
    "    if model_crps is not None or daily_profile_crps is not None:\n",
    "        if info_text:\n",
    "            info_text += r\"\\\\\"  # Line break in LaTeX\n",
    "        \n",
    "        if model_crps is not None:\n",
    "            info_text += r\"Model CRPS: \" + f\"{model_crps:.2f}\"\n",
    "        \n",
    "        if daily_profile_crps is not None:\n",
    "            if model_crps is not None:\n",
    "                info_text += r\"\\\\\"  # Line break in LaTeX\n",
    "            info_text += r\"Daily Profile CRPS: \" + f\"{daily_profile_crps:.2f}\"\n",
    "    \n",
    "    # If we have any info to display, create the text box\n",
    "    if info_text:\n",
    "        ax.text(0.99, 0.98, info_text, \n",
    "                transform=ax.transAxes, \n",
    "                fontsize=10, \n",
    "                ha='right', \n",
    "                va='top',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7),\n",
    "                multialignment='right')\n",
    "    \n",
    "    # Adjust bottom margin to make room for the legend\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 0.95])\n",
    "    \n",
    "    # Save the figure if requested\n",
    "    if save:\n",
    "        plt.savefig(filename, dpi=dpi, bbox_inches='tight')\n",
    "        print(f\"Figure saved as '{filename}' with DPI={dpi}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Helper function to find chunks of training and prediction data\n",
    "def get_chunks(df):\n",
    "    \"\"\"\n",
    "    Find continuous chunks of the same source type (training or prediction)\n",
    "    in a sorted dataframe. Returns a list of tuples (start_index, end_index, source_type).\n",
    "    \"\"\"\n",
    "    # Make sure the dataframe is sorted\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    chunks = []\n",
    "    current_source = None\n",
    "    start_idx = None\n",
    "    for idx, row in df.iterrows():\n",
    "        if current_source is None:\n",
    "            current_source = row['source']\n",
    "            start_idx = idx\n",
    "        elif row['source'] != current_source:\n",
    "            chunks.append((start_idx, idx, current_source))\n",
    "            current_source = row['source']\n",
    "            start_idx = idx\n",
    "    \n",
    "    # Add the last chunk\n",
    "    if current_source is not None and start_idx is not None:\n",
    "        chunks.append((start_idx, df.index[-1], current_source))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_random_period(df, period_length):\n",
    "    \"\"\"\n",
    "    Extract a random period of specified length from a datetime-indexed DataFrame.\n",
    "    The function ensures the extracted period always starts at the beginning of a day (00:00:00)\n",
    "    and ends at the end of a day (23:59:59.999999), and handles timezone-aware datetime objects.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame with datetime index\n",
    "    period_length : str or timedelta\n",
    "        Length of the period to extract (e.g., '7D' for 7 days, '3M' for 3 months)\n",
    "        Can be a string compatible with pandas.Timedelta or a timedelta object\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        A tuple containing (random_start, random_end) datetime objects\n",
    "        random_start will be at midnight (00:00:00) and random_end will be at end of day (23:59:59.999999)\n",
    "        Both preserve timezone information\n",
    "    \"\"\"\n",
    "    if isinstance(period_length, str):\n",
    "        period_length = pd.Timedelta(period_length)\n",
    "    \n",
    "    # Get the min and max dates of the dataframe\n",
    "    start_date = df.index.min()\n",
    "    end_date = df.index.max()\n",
    "    is_tz_aware = start_date.tzinfo is not None\n",
    "    \n",
    "    # Convert to midnight of first day\n",
    "    if is_tz_aware:\n",
    "        tz = start_date.tzinfo\n",
    "        start_date_midnight = pd.Timestamp.combine(start_date.date(), time(0, 0, 0)).tz_localize(tz)\n",
    "        # End date is the midnight of the next day after the last data point\n",
    "        end_date_midnight = pd.Timestamp.combine(end_date.date(), time(0, 0, 0)).tz_localize(tz)\n",
    "    else:\n",
    "        start_date_midnight = datetime.combine(start_date.date(), time(0, 0, 0))\n",
    "        end_date_midnight = datetime.combine(end_date.date(), time(0, 0, 0))\n",
    "    \n",
    "    # Calculate total range (in days)\n",
    "    # We need to adjust to get full days\n",
    "    days_in_period = period_length.days\n",
    "    \n",
    "    if days_in_period <= 0:\n",
    "        raise ValueError(f\"Period length must be at least 1 day, got {period_length}\")\n",
    "    \n",
    "    # Calculate the latest possible start date to have a full period\n",
    "    latest_start_date = end_date_midnight - pd.Timedelta(days=days_in_period-1)\n",
    "    \n",
    "    # Calculate total days available for random selection\n",
    "    days_available = (latest_start_date - start_date_midnight).days\n",
    "    \n",
    "    if days_available < 0:\n",
    "        raise ValueError(f\"Requested period length ({period_length}) is longer than available data range\")\n",
    "    \n",
    "    # Generate random start day\n",
    "    random_days = random.randint(0, days_available)\n",
    "    random_start = start_date_midnight + pd.Timedelta(days=random_days)\n",
    "    \n",
    "    # Calculate end date (end of the last day in the period)\n",
    "    random_end_date = (random_start + pd.Timedelta(days=days_in_period-1)).date()\n",
    "    \n",
    "    # Set random_end to the end of the day (23:59:59.999999)\n",
    "    if is_tz_aware:\n",
    "        end_of_day = pd.Timestamp.combine(\n",
    "            random_end_date, \n",
    "            time(23, 59, 59, 999999)\n",
    "        ).tz_localize(tz)\n",
    "    else:\n",
    "        end_of_day = datetime.combine(\n",
    "            random_end_date, \n",
    "            time(23, 59, 59, 999999)\n",
    "        )\n",
    "    \n",
    "    return random_start, end_of_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_prediction_df(config_ngb, \"CE\", \"f_rocof\")\n",
    "rand_start, rand_end = extract_random_period(df, '2D')\n",
    "rand_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in [config_ngb, config_tabnet_proba]:\n",
    "    df = get_prediction_df(config, area, \"f_rocof\")\n",
    "    random_period = df.loc[random_start:random_end]\n",
    "    filename = f\"figures_crps/{area}_{target}_{config.model_name}.pdf\"\n",
    "    plot_results(df=random_period, save=True, filename=filename, var_name=\"f_roxof\", model_name=config.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 4, 5, 6, 8. Relative SHAP Feature Importance Plot \n",
    "* Mean Prediction CE and Nordic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "def get_clustering(area, config, cutoff):\n",
    "    X_train = pd.read_hdf(\n",
    "                        data_folder.format(area, config.data_version, config.scaler_str)\n",
    "                        + f\"X_train_full{config.scaled}.h5\")\n",
    "    X_train.rename(columns=input_col_names, inplace=True)\n",
    "\n",
    "    correlation_matrix = X_train.corr(method='pearson').abs()\n",
    "    dist_matrix = 1 - correlation_matrix\n",
    "\n",
    "    clustering = linkage(dist_matrix, method=\"complete\", optimal_ordering=True)\n",
    "    clusters_cutoff = fcluster(clustering, cutoff, criterion='distance')\n",
    "    name_to_cluster = {name: cluster for name, cluster in zip(X_train.columns, clusters_cutoff)}\n",
    "   \n",
    "    return clustering, name_to_cluster\n",
    "\n",
    "\n",
    "def get_shap_df(area, config, target, shap_type):\n",
    "    X_test = pd.read_hdf(\n",
    "                        data_folder.format(area, config.data_version, config.scaler_str)\n",
    "                        + f\"X_test_full{config.scaled}.h5\")\n",
    "    X_test.rename(columns=input_col_names, inplace=True)\n",
    "    shap_vals = np.load(\n",
    "            explain_folder.format(area, config.res_version, config.model_combination, target, config.explanations)\n",
    "            + f\"shap_values_{shap_type}.npy\"\n",
    "        )\n",
    "    if len(shap_vals.shape) == 3 and shap_vals.shape[2] == 1:\n",
    "        shap_vals = shap_vals.reshape(shap_vals.shape[0], shap_vals.shape[1])\n",
    "\n",
    "    shap_vals = pd.DataFrame(\n",
    "                    data=shap_vals, index=X_test.index, columns=X_test.columns\n",
    "                )\n",
    "    return shap_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "def plot_shap_comparison_partition(configs: list[Config], area: str, shap_type: str, target: str, cutoff: float, save: bool = False):\n",
    "\n",
    "    shap_mean_importances: dict[str, dict[str, float]] = defaultdict(dict)\n",
    "    clustering, name_to_cluster = get_clustering(area,configs[0], cutoff=cutoff)\n",
    "    \n",
    "    names_by_cluster = defaultdict(list)\n",
    "\n",
    "    \n",
    "    for name, cluster in name_to_cluster.items():\n",
    "        names_by_cluster[cluster].append(name)\n",
    "\n",
    "    figpath_name = f\"../results/figures/comparison_plots/shap_comparison_{area}_{target}_{shap_type}_{str(cutoff)}\"\n",
    "    for conf in configs:\n",
    "        figpath_name += \"_\" + conf.model_name\n",
    "        shap_values = get_shap_df(area, conf, target, shap_type)\n",
    "\n",
    "        total_shap = 0 \n",
    "        for feature, shap_value in shap_values.items():\n",
    "            total_shap += shap_value.abs().mean()\n",
    "        for feature, shap_value in shap_values.items():\n",
    "            cluster = name_to_cluster[feature]\n",
    "            shap_mean_importances[cluster][conf.model_name] = shap_mean_importances[cluster].get(conf.model_name, 0) + shap_value.abs().mean() / total_shap\n",
    "\n",
    "    clusters = list(shap_mean_importances.keys())\n",
    "    clusters.sort(key= lambda x: max(shap_mean_importances[x].values()), reverse=True)\n",
    "\n",
    "    #for cluster in clusters:\n",
    "    #    print(\"\\nCluster \", cluster)\n",
    "    #    for feature in names_by_cluster[cluster]:\n",
    "    #       print(f\"\\t{feature}\")\n",
    "\n",
    "    plt.rc('text', usetex=True)\n",
    "    plt.rc('font', family='serif')\n",
    "\n",
    "    markers_by_model = {\"XGBoost\": \"x\", \"NGBoost\": \"o\", \"TabNet\": \"+\", \"TabnetProba\": \"*\"}\n",
    "    colors_by_model = {\"XGBoost\": \"tab:blue\", \"NGBoost\": \"tab:orange\", \"TabnetProba\": \"tab:green\", \"TabNet\": \"tab:red\"}\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.grid(True, which='both', axis='both', color='lightgrey', linestyle='-', linewidth=0.5)\n",
    "\n",
    "    #fig, ax = plt.figure()\n",
    "    k = 10\n",
    "\n",
    "    for i, cluster in enumerate(reversed(clusters[:k])):\n",
    "        for model, value in shap_mean_importances[cluster].items():\n",
    "            model_name = str(model)\n",
    "            if not i:\n",
    "                ax.scatter(value, i, c=colors_by_model[model], marker=markers_by_model[model], label=model_name)\n",
    "            else:\n",
    "                ax.scatter(value, i, c=colors_by_model[model], marker=markers_by_model[model])\n",
    "\n",
    "    ax.legend()\n",
    "    others = clusters[k:] \n",
    "    \n",
    "    clusters_labels = defaultdict(str)\n",
    "    # Create cluster names\n",
    "    if area == \"CE\":\n",
    "        cluster_mappings= {\n",
    "            frozenset([\"Gas ramp\", \"Reservoir hydro ramp\", \"Run-off-river hydro ramp\", \"Hard coal ramp\"]): \"Gas, hydro and hard coal ramps\",\n",
    "            frozenset([\"Biomass ramp\", \"Pumped hydro ramp\", \"Price ramp day-ahead\"]): \"Biomass, pumped hydro and prices DA ramps\",\n",
    "            frozenset([\"Load ramp\", \"Total generation ramp\", \"Load ramp day-ahead\", \"Generation ramp day-ahead\"]): \"Load and generation ramps cluster\",\n",
    "            frozenset([\"Solar ramp\", \"Forecast error generation ramp\", \"Solar ramp day-ahead\"]): \"Solar and forecast error gen. ramps cluster\",\n",
    "            frozenset([\"Solar generation\", \"Solar day-ahead\"]): \"Solar generation cluster\",\n",
    "            frozenset([\"Solar ramp\", \"Solar ramp day-ahead\"]): \"Solar ramp cluster\",\n",
    "            frozenset([\"Biomass generation\", \"Oil generation\"]): \"Biomass and oil gen. cluster\",\n",
    "            #frozenset([\"Nuclear generation\", \"Other renewable generation\"]): \"Other renewable gen. cluster\",\n",
    "            frozenset([\"Biomass generation\", \"Other renewable generation\"]): \"Other renewable gen. and biomass cluster\",\n",
    "        }\n",
    "    elif area == \"Nordic\":\n",
    "        cluster_mappings= {\n",
    "            frozenset([\"Load ramp\", \"Total generation ramp\", \"Run-off-river hydro ramp\",\"Reservoir hydro ramp\",\"Load ramp day-ahead\", \"Generation ramp day-ahead\"]): \"Load, gen. and hydro ramps cluster\",\n",
    "            frozenset([\"Load\", \"Reservoir hydro generation\",\"Total generation\",\"Synchronous generation\",\"Scheduled generation\",\"Load day-ahead\"]): \"Load, reservoir hydro and generation cluster\",\n",
    "            frozenset([\"Onshore wind ramp\", \"Onshore wind ramp day-ahead\"]): \"Onshore wind ramps cluster\",\n",
    "            frozenset([\"Solar generation\", \"Forecast error solar\",\"Solar day-ahead\"]): \"Solar features cluster\",\n",
    "            frozenset([\"Offshore wind ramp\", \"Forecast error offshore wind ramp\"]): \"Offshore wind cluster\"\n",
    "        }\n",
    "    \n",
    "    for cl in clusters:\n",
    "            cluster_features = frozenset(names_by_cluster[cl])\n",
    "            if cluster_features in cluster_mappings.keys():\n",
    "                print(\"Cluster features: \")\n",
    "                print(cluster_features)\n",
    "                print(\"Cluster mappings: \")\n",
    "                print(cluster_mappings[cluster_features])\n",
    "                clusters_labels[cl] = cluster_mappings[cluster_features]\n",
    "            \n",
    "            else:\n",
    "                clusters_labels[cl] = names_by_cluster[cl][0]\n",
    "\n",
    "    clusters_names = list(clusters_labels.values())\n",
    "    ax.set_yticks(list(range(k+1)), list(reversed(clusters_names[:k])) + [f\"Sum of {len(others)} other features\"], fontsize=12)\n",
    "\n",
    "    others_by_model = defaultdict(float)\n",
    "    for cluster in others:\n",
    "        for model, value in shap_mean_importances[cluster].items():\n",
    "            others_by_model[model] += value\n",
    "\n",
    "    for model in shap_mean_importances[cluster].keys():\n",
    "        ax.scatter(others_by_model[model], i+1, c=colors_by_model[model], marker=markers_by_model[model])\n",
    "\n",
    "    plt.title(r'\\textbf{SHAP Feature Importance for} \\textit{' + target + r'} \\textbf{in ' + area + '}', fontsize=14)\n",
    "    if shap_type == \"mean\":\n",
    "        plt.xlabel(r'Relative SHAP value (impact on mean prediction)', fontsize=14)\n",
    "    else:\n",
    "        plt.xlabel(r'Relative SHAP value (impact on uncertainty prediction)', fontsize=14)\n",
    "\n",
    "    figpath_name +=\".pdf\"\n",
    "    if save:\n",
    "        save_path = figpath_name\n",
    "        plt.savefig(save_path, bbox_inches='tight')  # Save the figure\n",
    "        print(f\"Figure saved at {save_path}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.show()\n",
    "\n",
    "configs_det = [config_xgb, config_tabnet]\n",
    "shap_type = \"mean\"\n",
    "area=\"CE\"\n",
    "target=\"f_rocof\"\n",
    "if area == \"CE\":\n",
    "    cutoff=0.8\n",
    "else:\n",
    "    cutoff=0.85\n",
    "\n",
    "configs_det = [config_xgb, config_tabnet]\n",
    "configs_proba = [config_ngb, config_tabnet_proba]\n",
    "configs_all = [config_xgb, config_tabnet, config_tabnet_proba, config_ngb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_comparison_partition(configs_proba, area, shap_type, target, cutoff=cutoff, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uncertainty Prediction CE and Nordic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 7, 9. Beeswarm Plot Uncertainty Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import pandas as pd\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from utils.plot_beeswarm import my_beeswarm\n",
    "\n",
    "def plot_shap_beeswarm_grid(configs, area, target, shap_type, names_by_cluster, \n",
    "                            sorted_clusters, num_clusters, save=False, clip_outliers=False):\n",
    "    X_test = pd.read_hdf(\n",
    "        data_folder.format(area, configs[0].data_version, configs[0].scaler_str)\n",
    "        + f\"X_test_full{configs[0].scaled}.h5\"\n",
    "    )\n",
    "    X_test.rename(columns=input_col_names, inplace=True)\n",
    "\n",
    "    num_models = len(configs)\n",
    "    size_const = 3.5\n",
    "    if num_clusters > 3:\n",
    "        size_const = 3\n",
    "    fig, axs = plt.subplots(nrows=num_clusters, ncols=num_models, figsize=(size_const*num_models, 2*num_clusters), \n",
    "                            squeeze=False, constrained_layout=True)\n",
    "    config_str = \"\".join([c.model_name for c in configs])\n",
    "    \n",
    "    # Define a custom colormap from blue to purple to red, similar to SHAP's\n",
    "    colors = [\"#1f77b4\", \"#9467bd\", \"#ff007f\"]  # Blue -> Purple -> Red\n",
    "    cmap = LinearSegmentedColormap.from_list(\"shap_custom\", colors)\n",
    "    \n",
    "    for row, cluster in enumerate(sorted_clusters[:num_clusters]):\n",
    "        for col, config in enumerate(configs):\n",
    "            features_in_cluster = names_by_cluster[cluster]\n",
    "            \n",
    "            # Get SHAP values for the current config and cluster\n",
    "            shap_values = get_shap_df(area, config, target, shap_type)\n",
    "            top_shap_vals = shap_values[features_in_cluster]\n",
    "            cluster_shap_values = shap.Explanation(top_shap_vals.values, feature_names=features_in_cluster, data=X_test[features_in_cluster])\n",
    "            \n",
    "            ax = axs[row, col]\n",
    "            my_beeswarm(ax=ax, shap_values=cluster_shap_values, show=False, plot_size=None, labels=not col, clip_outliers=clip_outliers)\n",
    "            ax.set_title(f\"Cluster {row + 1} - {config.model_name}\", fontsize=12)\n",
    "\n",
    "    # Create the color bar using the custom cmap\n",
    "    m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    m.set_array([0, 1])\n",
    "    \n",
    "    # Add the color bar on the side for a shared legend\n",
    "    cb = fig.colorbar(m, ax=axs, ticks=[0, 1], aspect=80, orientation=\"vertical\", fraction=0.2, pad=0.04)\n",
    "    cb.set_ticklabels([\"Low\", \"High\"])\n",
    "    cb.set_label(\"Feature Value\", size=12, labelpad=0)\n",
    "    cb.ax.tick_params(labelsize=11, length=0)\n",
    "    cb.set_alpha(1)\n",
    "    cb.outline.set_visible(False)\n",
    "\n",
    "    figpath_name = f\"../../results/figures/comparison_plots/beeswarm/{area}_{shap_type}_{config_str}_beeswarms{num_clusters}.pdf\"\n",
    "    if save:\n",
    "        plt.savefig(figpath_name, bbox_inches='tight')\n",
    "        print(f\"Figure saved at {figpath_name}\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_clusters(configs: list[Config], area: str, shap_type: str, target: str, cutoff: float):\n",
    "    shap_mean_importances: dict[str, dict[str, float]] = defaultdict(dict)\n",
    "    clustering, name_to_cluster = get_clustering(area, configs[0], cutoff)\n",
    "    \n",
    "    names_by_cluster = defaultdict(list)\n",
    "    \n",
    "    for name, cluster in name_to_cluster.items():\n",
    "        names_by_cluster[cluster].append(name)\n",
    "\n",
    "    for conf in configs:\n",
    "        shap_values = get_shap_df(area, conf, target, shap_type)\n",
    "  \n",
    "        total_shap = 0 \n",
    "        for feature, shap_value in shap_values.items():\n",
    "            total_shap += shap_value.abs().mean()\n",
    "\n",
    "        for feature, shap_value in shap_values.items():\n",
    "            cluster = name_to_cluster[feature]\n",
    "            shap_mean_importances[cluster][conf.model_name] = shap_mean_importances[cluster].get(conf.model_name, 0) + shap_value.abs().mean() / total_shap\n",
    "    \n",
    "    sorted_clusters = list(shap_mean_importances.keys())\n",
    "    sorted_clusters.sort(key= lambda x: max(shap_mean_importances[x].values()), reverse=True)\n",
    "    \n",
    "    return clustering, sorted_clusters, names_by_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beeswarm Plot for CE RoCoF\n",
    "shap_type = \"std\"\n",
    "area=\"CE\"\n",
    "target=\"f_rocof\"\n",
    "cutoff = 0.8\n",
    "\n",
    "num_clusters = 1# Set number of clusters to display\n",
    "clustering, sorted_clusters, names_by_cluster = get_sorted_clusters(configs_proba, area, shap_type, target, cutoff=cutoff)\n",
    "plot_shap_beeswarm_grid(configs_proba, area, target, shap_type, names_by_cluster, sorted_clusters, num_clusters, save=False, clip_outliers=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beeswarm Plot for Nordic Nadir\n",
    "shap_type = \"std\"\n",
    "area=\"Nordic\"\n",
    "target=\"f_ext\"\n",
    "cutoff = 0.85\n",
    "\n",
    "num_clusters = 2# Set number of clusters to display\n",
    "clustering, sorted_clusters, names_by_cluster = get_sorted_clusters(configs_proba, area, shap_type, target, cutoff=cutoff)\n",
    "plot_shap_beeswarm_grid(configs_proba, area, target, shap_type, names_by_cluster, sorted_clusters, num_clusters, save=False, clip_outliers=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "## Figure 10 - Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_double_correlation_heatmap(save=False, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Plots a double heatmap for the CE and Nordic areas, showing only correlations\n",
    "    with absolute value greater than or equal to the specified threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - threshold: float, the threshold for filtering correlations (default: 0.7).\n",
    "    \"\"\"\n",
    "    plt.rc('text', usetex=True)\n",
    "    plt.rc('font', family='serif')\n",
    "    data_version = \"2024-05-19\"\n",
    "    \n",
    "    areas = [\"CE\", \"Nordic\"]\n",
    "    figure_path = \"../probabilistic-XAI-for-grid-frequency-stability/results/figures/correlation/\"\n",
    "    \n",
    "    if not os.path.exists(figure_path):\n",
    "        os.makedirs(figure_path)\n",
    "        \n",
    "    _, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 16), sharex=True)\n",
    "\n",
    "    for i, area in enumerate(areas):\n",
    "        version_folder = f\"../data/2020-2024/{area}/version_{data_version}/yeo_johnson/\"\n",
    "        X_train = pd.read_hdf(version_folder + \"X_train_full_scaled.h5\")\n",
    "        X_train.rename(columns=input_col_names, inplace=True)\n",
    "\n",
    "        # Calculate and filter correlation matrix\n",
    "        correlation_matrix = X_train.corr(method='pearson')\n",
    "        filtered_corr_matrix = correlation_matrix.copy()\n",
    "        filtered_corr_matrix[np.abs(filtered_corr_matrix) < threshold] = 0\n",
    "\n",
    "        non_zero_columns = filtered_corr_matrix.columns[\n",
    "            (np.abs(filtered_corr_matrix).sum(axis=0) - np.abs(np.diag(filtered_corr_matrix))) > 0\n",
    "        ]\n",
    "        filtered_corr_matrix = filtered_corr_matrix.loc[non_zero_columns, non_zero_columns]\n",
    "\n",
    "        sns.heatmap(\n",
    "            filtered_corr_matrix, annot=False, cmap=\"coolwarm\", ax=axes[i],\n",
    "            linewidths=.5, vmin=-1, vmax=1, cbar_kws={'label': 'Pearson Correlation' if i == 1 else None}\n",
    "        )\n",
    "        \n",
    "        axes[i].set_title(r'Correlation Heatmap of ' + area + r' Features ($|corr| \\geq ' + str(threshold) + r'$)', fontsize=16)\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        for tick in axes[i].get_xticklabels():\n",
    "            tick.set_horizontalalignment('right')\n",
    "            tick.set_verticalalignment('center_baseline')\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(figure_path + \"double_corr_heatmap.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_double_correlation_heatmap(threshold=0.7, save=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 12 - Performance Overview of All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 13, 14 - Calibration Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_calibration_curves(area, save: bool):\n",
    "    \"\"\"\n",
    "    Function to plot calibration curves for different areas and targets based on the provided config.\n",
    "    \n",
    "    Args:\n",
    "    - config: An object containing necessary configurations such as data folder paths, \n",
    "              list of areas, targets, and other model-related settings.\n",
    "    \"\"\"\n",
    "    plt.rcParams['text.usetex'] = True\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "    plt.rcParams['font.size'] = 12\n",
    "\n",
    "    areas = [\"CE\", \"Nordic\"]\n",
    "    targets = [\"f_rocof\", \"f_ext\", \"f_msd\", \"f_integral\"]\n",
    "\n",
    "    target_mappings = {\n",
    "        \"f_rocof\": \"RoCoF\",\n",
    "        \"f_ext\": \"Nadir\",\n",
    "        \"f_msd\": \"MSD\",\n",
    "        \"f_integral\": \"Integral\"\n",
    "    }\n",
    "\n",
    "    confidence_levels = np.arange(0.1, 1.0, 0.1).tolist() + [0.99]  \n",
    "    z_values = [norm.ppf((1 + level) / 2) for level in confidence_levels]  \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11, 5))\n",
    "    axes = dict(zip([\"NGBoost\", \"TabnetProba\"], axes))  \n",
    "\n",
    "    for config in [config_ngb, config_tabnet_proba]:\n",
    "        for target in targets:\n",
    "            X_test = pd.read_hdf(data_folder.format(area, config.data_version) + \"X_test_full_scaled.h5\")\n",
    "            y_test = pd.read_hdf(data_folder.format(area, config.data_version) + f\"y_test{config.scaled}.h5\").loc[:, target]\n",
    "            y_pred = pd.read_hdf(fit_folder.format(area, config.res_version, config.model_combination, config.scaler_str, target) + \"y_pred.h5\")\n",
    "            if config.model_name == \"NGBoost\":\n",
    "\n",
    "                ngb_model = joblib.load(fit_folder.format(area, config.res_version, config.model_combination, config.scaler_str, target) + \"best_ngb_model.pkl\")\n",
    "\n",
    "                y_pred_ngb = ngb_model.pred_dist(X_test)\n",
    "                y_pred_mean = y_pred[\"predictions\"]\n",
    "                y_pred_std = y_pred_ngb.scale\n",
    "            elif config.model_name == \"TabnetProba\":\n",
    "                model = TabNetRegressorProba()\n",
    "                model.load_model(fit_folder.format(area, config.res_version, config.model_combination, config.scaler_str, target) + \"best_model_params.zip\")\n",
    "                y_pred_tabnet = model.predict(X_test.values)\n",
    "                y_pred_mean = y_pred_tabnet[:, 0]\n",
    "                y_pred_std = np.sqrt(y_pred_tabnet[:, 1]) + 1e-6\n",
    "            \n",
    "            y_true = y_test.values\n",
    "\n",
    "            observed_coverage = []\n",
    "\n",
    "            for z in z_values:\n",
    "                lower_bounds = y_pred_mean - z * y_pred_std\n",
    "                upper_bounds = y_pred_mean + z * y_pred_std\n",
    "                within_interval = np.logical_and(y_true >= lower_bounds, y_true <= upper_bounds)\n",
    "                observed_coverage.append(np.mean(within_interval))\n",
    "\n",
    "            axes[config.model_name].plot(confidence_levels, observed_coverage, marker='o', label=target_mappings[target])\n",
    "\n",
    "        axes[config.model_name].plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect calibration')\n",
    "        axes[config.model_name].set_xlabel('Expected confidence level', fontsize=14)\n",
    "        axes[config.model_name].set_ylabel('Observed coverage', fontsize=14)\n",
    "        axes[config.model_name].set_title(f'Prediction Interval Calibration - {config.model_name}')\n",
    "        axes[config.model_name].legend(title=\"Targets\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        figpath_name = f\"../../results/figures/comparison_plots/calibration_curve_{area}.pdf\"\n",
    "        plt.savefig(figpath_name, bbox_inches='tight')  # Save the figure\n",
    "        print(f\"Figure saved at {figpath_name}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "#plot_calibration_curves(\"CE\", save=False)\n",
    "\n",
    "#plot_calibration_curves(\"Nordic\", save=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 15, 16 - Clustering Dendrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_clustering_cutoff(area,  cutoff : float,  data_version=\"2024-05-19\", input_cols=None, save=False):\n",
    "    version_folder = f\"../data/2020-2024/{area}/version_{data_version}/yeo_johnson/\"\n",
    "    X_train = pd.read_hdf(version_folder + \"X_train_full_scaled.h5\")\n",
    "    \n",
    "    figure_path = (\n",
    "        f\"../../probabilistic-XAI-for-grid-frequency-stability/results/figures/correlation/{area}/\"\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(figure_path):\n",
    "        os.makedirs(figure_path)\n",
    "    \n",
    "    correlation_matrix = X_train.corr(method='pearson')\n",
    "\n",
    "    correlation_matrix_abs = np.abs(correlation_matrix)\n",
    "    dist_matrix_df = 1 - correlation_matrix_abs\n",
    "\n",
    "    clustering = sch.linkage(dist_matrix_df, method=\"complete\", optimal_ordering=True)\n",
    "    \n",
    "    if input_cols:\n",
    "        labels = [input_cols.get(col, col) for col in X_train.columns]\n",
    "    else:\n",
    "        labels = X_train.columns\n",
    "    \n",
    "    plt.rc('text', usetex=True)\n",
    "    plt.rc('font', family='serif')\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.title(r\"\\textbf{Dendrogram of Correlation Clustering of \" + area + ' Features}', fontsize=14)\n",
    "    plt.axhline(y=cutoff, c='k', lw=1.5, linestyle='--')\n",
    "    plt.text(0.95, cutoff, f'cutoff:{cutoff}', va='center_baseline', ha='right', color='k', fontsize=10)\n",
    "\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tick_params(axis='x', which='major', labelsize=12)\n",
    "    plt.ylabel(r'\\textbf{Correlation Distance}', fontsize=14)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(figure_path + f\"pearson_corr_{area}_{cutoff}.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_Nordic= plot_corr_clustering_cutoff(area=\"Nordic\", cutoff=0.85, input_cols=input_col_names, save=False)\n",
    "clustering_CE = plot_corr_clustering_cutoff(area=\"CE\", cutoff=0.8, input_cols=input_col_names, save=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
